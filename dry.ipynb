{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dry Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "We learned that given a space $R^d$ the vector $w\\in R^d$ defines a homogeneous halfspace of the space using the classification function $f(x)=sign(\\langle x,w\\rangle)$\n",
    "\n",
    "We would like to find a function $g:R^d\\rightarrow R^{d+1}$ which will define a non-homogeneous halfspace of $R^d$.\n",
    "\n",
    "Therefore, for all $x\\in R^d$ such that $x=(x_1, x_2, \\ldots, x_d)$ we will define  $g:R^d\\rightarrow R^{d+1}$ by $g(x)=(1, x_1, x_2, \\ldots, x_d)$.\n",
    "\n",
    "For all super-plane which induces a non-homogeneous halfspace of $R^d$, that is defined by $z\\in R^{d+1}$ we can take the value $z_1$, which is actually can be seen as moving the first coordinate by $z_1$ from the origin.\n",
    "\n",
    "Therfore, there exists a vector $w \\in R^d$ such that $w=(z_2,z_3,\\ldots,z_{d+1})$ which defines a super-plane that induces a non-homogeneous halfspace of $R^d$, and is parallel to the plane which is defined by $z$.\n",
    "\n",
    "Thus, $f(x)=sign(\\langle g(x), z \\rangle)$ defines the non-homogeneous halfspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "For dataset A:  \n",
    "\n",
    "1. Perceptron - cannot fit perfectly as the dataset is not linearly seperable. You might be able to separate the data in a higher dimension by adding features.\n",
    "2. KNN with k=1 - can fit perfectly as each sample in training set will be classified by itself\n",
    "3. KNN with k=3 - might not fit perfectly as there are samples close enough to two other samples from the other color.\n",
    "4. Decision Tree - can fit perfectly as you can separate the data into 4 quadrons which can be labeled using only 4 leafs.\n",
    "\n",
    "For Dataset B:\n",
    "\n",
    "1. Perceptron - can fit perfectly, as the dataset is clearly linearly seperable.\n",
    "2. KNN with k=1 - can fit perfectly as each sample in training set will be classified by itself\n",
    "3. KNN with k=3 - cannot fit perfectly as there are samples which are close to two other samples from the other color, and as such they will be classified wrongfuly.\n",
    "4. Decision Tree - cannot fit perfectly, because you'll need more than 4 leafs to separate the data on the \"diagonal\" between classes\n",
    "\n",
    "For Dataset C:\n",
    "\n",
    "1. Perceptron - cannot fit perfectly, as the dataset is not linearly seperable. You might be able to separate the data in a higher dimension by adding features.\n",
    "2. KNN with k=1 - can fit perfectly as each sample in training set will be classified by itself\n",
    "3. KNN with k=3 - can fit perfectly because all samples are only close to two other samples by the same color, and as such they will be classified correctly.\n",
    "4. Decision Tree - cannot fit perfectly on the given features because you'll need more than 4 leafs the seperate the \"circle\" which is not a linear horizontal or vertical line. If we'll add a feature $x^2 + y^2$ we can use a decision tree (and perceptron for that matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Part 1:\n",
    "\n",
    "We will derive $\\frac{\\partial}{\\partial w_j}l(x_i,y_i,w)$, using the known sigmoid derviative $\\sigma(z)'=\\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j}l(x_i,y_i,w) = \\frac{\\partial}{\\partial \\sigma}l(x_i,y_i,w)\\cdot\\frac{\\partial}{\\partial w_j}\\sigma(y_i\\langle x_i,w\\rangle) \\\\\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\sigma(y_i\\langle x_i,w\\rangle)}\\cdot\\frac{\\partial}{\\partial w_j}\\sigma(y_i\\langle x_i,w\\rangle) \\\\\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\sigma(y_i\\langle x_i,w\\rangle)}\\cdot\\sigma(y_i\\langle x_i,w\\rangle)\\cdot(1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot \\frac{\\partial(yi\\cdot\\langle x_i w\\rangle)}{\\partial w_j} \\\\\n",
    "$$\n",
    "$$\n",
    "= (1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_{i,j}\n",
    "$$\n",
    "\n",
    "Part 2:\n",
    "\n",
    "We will derive w.r.t $w$ using the above derviative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}l(x_i,y_i,w) = \\begin{bmatrix}\\frac{\\partial}{\\partial w_1}l(x_i,y_i,w) & \\ldots & \\frac{\\partial}{\\partial w_n}l(x_i,y_i,w)\\end{bmatrix} \\\\\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix}(1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_{i,1} & \\ldots & (1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_{i,n}\\end{bmatrix} \\\\\n",
    "$$\n",
    "$$\n",
    "= (1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_i\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
