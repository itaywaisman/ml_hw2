{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dry Part"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 2\n",
    "\n",
    "For dataset A:  \n",
    "\n",
    "1. Perceptron - cannot fit perfectly as the dataset is not linearly seperable. You might be able to separate the data in a higher dimension by adding features.\n",
    "2. KNN with k=1 - can fit perfectly as each sample in training set will be classified by itself\n",
    "3. KNN with k=3 - might not fit perfectly as there are samples close enough to two other samples from the other color.\n",
    "4. Decision Tree - can fit perfectly as you can separate the data into 4 quadrons which can be labeled using only 4 leafs.\n",
    "\n",
    "For Dataset B:\n",
    "\n",
    "1. Perceptron - can fit perfectly, as the dataset is clearly linearly seperable.\n",
    "2. KNN with k=1 - can fit perfectly as each sample in training set will be classified by itself\n",
    "3. KNN with k=3 - cannot fit perfectly as there are samples which are close to two other samples from the other color, and as such they will be classified wrongfuly.\n",
    "4. Decision Tree - cannot fit perfectly, because you'll need more than 4 leafs to separate the data on the \"diagonal\" between classes\n",
    "\n",
    "For Dataset C:\n",
    "\n",
    "1. Perceptron - cannot fit perfectly, as the dataset is not linearly seperable. You might be able to separate the data in a higher dimension by adding features.\n",
    "2. KNN with k=1 - can fit perfectly as each sample in training set will be classified by itself\n",
    "3. KNN with k=3 - can fit perfectly because all samples are only close to two other samples by the same color, and as such they will be classified correctly.\n",
    "4. Decision Tree - cannot fit perfectly on the given features because you'll need more than 4 leafs the seperate the \"circle\" which is not a linear horizontal or vertical line. If we'll add a feature $x^2 + y^2$ we can use a decision tree (and perceptron for that matter)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 3\n",
    "\n",
    "Part 1:\n",
    "\n",
    "We will derive $\\frac{\\partial}{\\partial w_j}l(x_i,y_i,w)$, using the known sigmoid derviative $\\sigma(z)'=\\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j}l(x_i,y_i,w) = \\frac{\\partial}{\\partial \\sigma}l(x_i,y_i,w)\\cdot\\frac{\\partial}{\\partial w_j}\\sigma(y_i\\langle x_i,w\\rangle) \\\\\n",
    "= \\frac{1}{\\sigma(y_i\\langle x_i,w\\rangle)}\\cdot\\frac{\\partial}{\\partial w_j}\\sigma(y_i\\langle x_i,w\\rangle) \\\\\n",
    "= \\frac{1}{\\sigma(y_i\\langle x_i,w\\rangle)}\\cdot\\sigma(y_i\\langle x_i,w\\rangle)\\cdot(1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot \\frac{\\partial(yi\\cdot\\langle x_i w\\rangle)}{\\partial w_j} \\\\\n",
    "= (1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_{i,j}\n",
    "$$\n",
    "\n",
    "Part 2:\n",
    "\n",
    "We will derive w.r.t $w$ using the above derviative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}l(x_i,y_i,w) = \\begin{bmatrix}\\frac{\\partial}{\\partial w_1}l(x_i,y_i,w) & \\ldots & \\frac{\\partial}{\\partial w_n}l(x_i,y_i,w)\\end{bmatrix} \\\\\n",
    "= \\begin{bmatrix}(1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_{i,1} & \\ldots & (1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_{i,n}\\end{bmatrix} \\\\\n",
    "= (1-\\sigma(y_i\\langle x_i,w\\rangle))\\cdot y_i \\cdot x_i\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}