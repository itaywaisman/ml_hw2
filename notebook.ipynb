{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_figures = True\n",
    "\n",
    "def display_correlation_matrix(X,y):\n",
    "    if not display_figures:\n",
    "        return\n",
    "\n",
    "    Xy = pd.concat([X, y], axis=1)\n",
    "    corr = Xy.corr()\n",
    "    f = plt.figure(figsize=(20, 20))\n",
    "    plt.matshow(corr, fignum=f.number)\n",
    "    plt.xticks(range(Xy.shape[1]), Xy.columns, fontsize=14, rotation=90)\n",
    "    plt.yticks(range(Xy.shape[1]), Xy.columns, fontsize=14)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title('Correlation Matrix', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data analysis and manipulation tool\n",
    "import numpy as np # Numerical computing tools\n",
    "import matplotlib.pyplot as plt # another visualization library\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from skrebate import ReliefF\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file = 'virus_hw2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(labels=['Address', 'Job', 'PatientID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Split to train, validate and test sets\n",
    "\n",
    "The ratio is 70% for training, 15% for validating and 15% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(labels=['TestResultsCode'], axis=1)\n",
    "y = df[['TestResultsCode']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.176, random_state=1) # 0.176 x 0.85 = 0.15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Fix the data types\n",
    "\n",
    "Fixing CurrentLocation lat long type - separate into 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_categories = [\n",
    "    'not_detected_Spreader_NotatRisk',\n",
    "    'not_detected_NotSpreader_atRisk',\n",
    "    'not_detected_NotSpreader_NotatRisk',\n",
    "    'not_detected_Spreader_atRisk',\n",
    "    'cold_NotSpreader_NotatRisk',\n",
    "    'cold_Spreader_NotatRisk',\n",
    "    'cold_Spreader_atRisk',\n",
    "    'cold_NotSpreader_atRisk',\n",
    "    'flue_NotSpreader_NotatRisk',\n",
    "    'flue_NotSpreader_atRisk',\n",
    "    'flue_Spreader_NotatRisk',\n",
    "    'covid_NotSpreader_atRisk',\n",
    "    'covid_Spreader_NotatRisk',\n",
    "    'flue_Spreader_atRisk',\n",
    "    'covid_NotSpreader_NotatRisk',\n",
    "    'covid_Spreader_atRisk',\n",
    "    'cmv_NotSpreader_NotatRisk',\n",
    "    'cmv_Spreader_atRisk',\n",
    "    'cmv_NotSpreader_atRisk',\n",
    "    'cmv_Spreader_NotatRisk',\n",
    "    'measles_Spreader_NotatRisk',\n",
    "    'measles_NotSpreader_NotatRisk',\n",
    "    'measles_NotSpreader_atRisk',\n",
    "    'measles_Spreader_atRisk',\n",
    "]\n",
    "\n",
    "convert_features_dict = {\n",
    "    'BloodType': pd.CategoricalDtype(categories=['AB-', 'A+', 'AB+', 'A-', 'B-', 'O-', 'B+', 'O+']),\n",
    "    'SyndromeClass': pd.CategoricalDtype(categories=range(1,5))\n",
    "}\n",
    "\n",
    "convert_label_dict = {\n",
    "    'TestResultsCode': pd.CategoricalDtype(categories=label_categories)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fix_data_types(X):\n",
    "    return X.astype(convert_features_dict)\n",
    "\n",
    "data_types_transofrmer = FunctionTransformer(fix_data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fix_label_type(y):\n",
    "    y = y.astype(convert_label_dict)\n",
    "#     y.update(y[['TestResultsCode']].apply(lambda x: x.cat.codes))\n",
    "#     y = y.astype({'TestResultsCode': int})\n",
    "    return y\n",
    "\n",
    "label_transformer = FunctionTransformer(fix_label_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_sex_type(X):\n",
    "    return X.replace({'Sex': {'F': -1, 'M': 1}})\n",
    "\n",
    "sex_type_transformer = FunctionTransformer(handle_sex_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle BloodType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_blood_type(X):\n",
    "    return pd.get_dummies(X, columns=[\"BloodType\"], prefix=[\"BloodType\"])\n",
    "\n",
    "blood_type_transformer = FunctionTransformer(handle_blood_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle DateOfPCRTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_date_of_pcr_test(X):\n",
    "    X['DateOfPCRTest'] = pd.to_datetime(X['DateOfPCRTest'], infer_datetime_format=True)\n",
    "    X['DateOfPCRTest'] = X['DateOfPCRTest'].values.astype(float)\n",
    "    X['DateOfPCRTest'].values[X['DateOfPCRTest'].values < 0] = np.nan\n",
    "    return X\n",
    "\n",
    "date_of_pcr_test_type_transformer = FunctionTransformer(handle_date_of_pcr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle SyndromeClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_syndrome_class(X):\n",
    "    return pd.get_dummies(X, columns=[\"SyndromeClass\"], prefix=[\"SyndromeClass\"])\n",
    "\n",
    "syndrome_class_transformer = FunctionTransformer(handle_syndrome_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle location\n",
    "\n",
    "we separate the location into 2 features for longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_location(X):\n",
    "    long_lat_df = X['CurrentLocation'].str.strip('(Decimal').str.split(', ', expand=True).rename(columns={0:'Lat', 1:'Long'})\n",
    "    X['CurrentLocation_Lat'] = long_lat_df['Lat'].str.strip(\"')\")\n",
    "    X['CurrentLocation_Long'] = long_lat_df['Long'].str.strip(\"Decimal('\").str.rstrip(\"'))\")\n",
    "\n",
    "    convert_dict = {\n",
    "        'CurrentLocation_Lat': float,\n",
    "        'CurrentLocation_Long': float,\n",
    "    }\n",
    "\n",
    "    X = X.astype(convert_dict)\n",
    "    return X.drop(labels=['CurrentLocation'], axis=1)\n",
    "\n",
    "location_transformer = FunctionTransformer(handle_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle symptoms\n",
    "\n",
    "We create a one-hot vector of all symptoms and remove the old mixed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_symptoms(X):\n",
    "    splitted_df = X['SelfDeclarationOfIllnessForm'].str.split(';', expand=True)\n",
    "    values = splitted_df.values.flatten()\n",
    "    unique_values = pd.unique(values).tolist()\n",
    "    stripped_unique_values = [str(val).strip(' ') for val in unique_values]\n",
    "\n",
    "    # Split by ; to create a list for each row\n",
    "    X['SelfDeclarationOfIllnessForm_list'] = X['SelfDeclarationOfIllnessForm'].str.split(';')\n",
    "\n",
    "    # Replace NAN values with empty list\n",
    "    isna = X['SelfDeclarationOfIllnessForm_list'].isna()\n",
    "    X.loc[isna, 'SelfDeclarationOfIllnessForm_list'] = pd.Series([['nan']] * isna.sum()).values\n",
    "\n",
    "    # strip whitespaces\n",
    "    X['SelfDeclarationOfIllnessForm_list'] = [[str(val).strip() for val in list(symptom_list)]\n",
    "                                              for symptom_list in X['SelfDeclarationOfIllnessForm_list'].values]\n",
    "\n",
    "    # Create columns\n",
    "    for column_name in stripped_unique_values:\n",
    "        X[column_name] = X['SelfDeclarationOfIllnessForm_list'].map(lambda l: 1 if column_name in l else 0)\n",
    "\n",
    "    # Rename no symptoms column\n",
    "    # Drop irrelevant features\n",
    "    X = X.rename(columns={'nan': 'No_Symptoms'})\\\n",
    "        .drop(labels=['SelfDeclarationOfIllnessForm','SelfDeclarationOfIllnessForm_list'], axis=1)\n",
    "    return X\n",
    "\n",
    "symptoms_transformer = FunctionTransformer(handle_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_data_types_pipeline = Pipeline([\n",
    "    ('handle_sex_type', sex_type_transformer),\n",
    "    ('handle_blood_type', blood_type_transformer),\n",
    "    ('handle_date_of_pcr_test', date_of_pcr_test_type_transformer),\n",
    "    ('handle_syndrome_class', syndrome_class_transformer),\n",
    "    ('handle_location', location_transformer),\n",
    "    ('handle_symptoms', symptoms_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    'AvgHouseholdExpenseOnPresents', \n",
    "    'AvgHouseholdExpenseOnSocialGames',\n",
    "    'AvgHouseholdExpenseParkingTicketsPerYear',\n",
    "    'AvgMinSportsPerDay',\n",
    "    'AvgTimeOnSocialMedia',\n",
    "    'AvgTimeOnStuding',\n",
    "    'BMI',\n",
    "    'DateOfPCRTest',\n",
    "    'NrCousins',\n",
    "    'StepsPerYear',\n",
    "    'TimeOnSocialActivities',\n",
    "    'pcrResult1',\n",
    "    'pcrResult2',\n",
    "    'pcrResult3',\n",
    "    'pcrResult4',\n",
    "    'pcrResult5',\n",
    "    'pcrResult6',\n",
    "    'pcrResult7',\n",
    "    'pcrResult8',\n",
    "    'pcrResult9',\n",
    "    'pcrResult10',\n",
    "    'pcrResult11',\n",
    "    'pcrResult12',\n",
    "    'pcrResult13',\n",
    "    'pcrResult14',\n",
    "    'pcrResult15',\n",
    "    'pcrResult16',\n",
    "    'CurrentLocation_Lat',\n",
    "    'CurrentLocation_Long']\n",
    "\n",
    "\n",
    "categorical_features = [\n",
    "    'AgeGroup',\n",
    "    'DisciplineScore', \n",
    "    'HappinessScore', \n",
    "    'Sex',\n",
    "]\n",
    "\n",
    "\n",
    "class Imputer:\n",
    "    def __init__(self):\n",
    "        self.iterative_imputer = IterativeImputer(initial_strategy='median')\n",
    "        self.knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    def fit(self, X, y, **kargs):\n",
    "        self.iterative_imputer.fit(X[numeric_features])\n",
    "        self.knn_imputer.fit(X)\n",
    "        print(X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[numeric_features] = self.iterative_imputer.transform(X[numeric_features])\n",
    "        res = self.knn_imputer.transform(X)\n",
    "        X = pd.DataFrame(res, columns=X.columns)\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X, y, **kwargs):\n",
    "        self.fit(X,y, **kwargs)\n",
    "        return self.transform(X)\n",
    "\n",
    "imputation_transformer = Imputer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OutlierClipper transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierClipper:\n",
    "    def __init__(self, features):\n",
    "        self._features = features\n",
    "        self._feature_map = {}\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        df = X[self._features]\n",
    "        features = list(df.columns)\n",
    "        for feature in features:\n",
    "            f_q1 = df[feature].quantile(0.25)\n",
    "            f_q3 = df[feature].quantile(0.75)\n",
    "            f_iqr = f_q3 - f_q1\n",
    "            self._feature_map[feature] = (f_q1 - (1.5 * f_iqr), f_q3 + (1.5 * f_iqr))\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_copy = data.copy()\n",
    "        for feature in self._feature_map.keys():\n",
    "            data_copy[feature] = data_copy[feature].clip(lower=self._feature_map[feature][0],\n",
    "                                                         upper=self._feature_map[feature][1])\n",
    "        return data_copy\n",
    "\n",
    "    def fit_transform(self, X, y, **kwargs):\n",
    "        self.fit(X, y, **kwargs)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_features = ['StepsPerYear',\n",
    "                    'TimeOnSocialActivities',\n",
    "                    'AvgHouseholdExpenseOnPresents',\n",
    "                    'AvgHouseholdExpenseOnSocialGames',\n",
    "                    'AvgHouseholdExpenseParkingTicketsPerYear',\n",
    "                    'AvgMinSportsPerDay',\n",
    "                    'AvgTimeOnSocialMedia',\n",
    "                    'AvgTimeOnStuding',\n",
    "                    'BMI',\n",
    "                    'pcrResult1',\n",
    "                    'pcrResult10',\n",
    "                    'pcrResult11',\n",
    "                    'pcrResult12',\n",
    "                    'pcrResult13',\n",
    "                    'pcrResult14',\n",
    "                    'pcrResult15',\n",
    "                    'pcrResult16',\n",
    "                    'pcrResult2',\n",
    "                    'pcrResult3',\n",
    "                    'pcrResult4',\n",
    "                    'pcrResult5',\n",
    "                    'pcrResult6',\n",
    "                    'pcrResult7',\n",
    "                    'pcrResult8',\n",
    "                    'pcrResult9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_transformer = OutlierClipper(features=continous_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_scaled_features = ['AgeGroup',\n",
    "    'AvgHouseholdExpenseOnPresents',\n",
    "    'AvgHouseholdExpenseOnSocialGames',\n",
    "    'AvgHouseholdExpenseParkingTicketsPerYear',\n",
    "    'AvgMinSportsPerDay',\n",
    "    'AvgTimeOnSocialMedia',\n",
    "    'AvgTimeOnStuding',\n",
    "    'BMI',\n",
    "    'DateOfPCRTest',\n",
    "    'DisciplineScore',\n",
    "    'HappinessScore',\n",
    "    'NrCousins',\n",
    "    'StepsPerYear',\n",
    "    'TimeOnSocialActivities',\n",
    "    'pcrResult14',\n",
    "    'pcrResult16']\n",
    "\n",
    "negative_scaled_features = [\n",
    "    'pcrResult1',\n",
    "    'pcrResult10',\n",
    "    'pcrResult11',\n",
    "    'pcrResult12',\n",
    "    'pcrResult13',\n",
    "    'pcrResult15',\n",
    "    'pcrResult2',\n",
    "    'pcrResult3',\n",
    "    'pcrResult4',\n",
    "    'pcrResult5',\n",
    "    'pcrResult6',\n",
    "    'pcrResult7',\n",
    "    'pcrResult8',\n",
    "    'pcrResult9',\n",
    "]\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.min_max_scaler = MinMaxScaler()\n",
    "        self.max_abs_scaler = MaxAbsScaler()\n",
    "\n",
    "    def fit(self, X, y, **kargs):\n",
    "        self.min_max_scaler.fit(X[positive_scaled_features])\n",
    "        self.max_abs_scaler.fit(X[negative_scaled_features])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kargs):\n",
    "        X[positive_scaled_features] = self.min_max_scaler.transform(X[positive_scaled_features])\n",
    "        X[negative_scaled_features] = self.max_abs_scaler.transform(X[negative_scaled_features])\n",
    "        \n",
    "        X['CurrentLocation_Lat'] /= 180\n",
    "        X['CurrentLocation_Long'] /= 180\n",
    "        \n",
    "        return X\n",
    "    \n",
    "normalize_transformer = Normalizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = ['cmv', 'cold', 'covid', 'flue', 'measles', 'not_detected']\n",
    "labels = pd.DataFrame()\n",
    "labels['is_spreader'] = pd.Series([0 if 'NotSpreader' in row else 1 for row in df['TestResultsCode']])\n",
    "labels['is_at_risk'] = pd.Series([0 if 'NotatRisk' in row else 1 for row in df['TestResultsCode']])\n",
    "for d in diseases:\n",
    "    labels[d] = pd.Series([1 if d in row else 0 for row in df['TestResultsCode']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_filter(X, y):\n",
    "    relief = RFE(ReliefF(), n_features_to_select=20, step = 0.5, verbose=True)\n",
    "    feature_selector = relief.fit(X, y['TestResultsCode'].values.codes)\n",
    "    print('Selected Features:', X.columns[feature_selector.support_])\n",
    "    return feature_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import LinearSVC## Feature selection with Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_wrapper(X,y):\n",
    "#     svc = SVC(gamma='auto')\n",
    "#     linearSVC = LinearSVC(random_state=0, tol=1e-5, class_weight='balanced')\n",
    "    random_forest_clssifier = RandomForestClassifier(max_depth=7, random_state=0)\n",
    "    # knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    sfs = SequentialFeatureSelector(random_forest_clssifier, k_features=25, forward=False, floating=False, verbose=5, cv=0, n_jobs=-1)\n",
    "    sfs.fit(X, y)\n",
    "\n",
    "    print(sfs.k_feature_names_)\n",
    "\n",
    "    return sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_preperation_pipelines = Pipeline([\n",
    "    ('feature_types', features_data_types_pipeline),\n",
    "    ('feature_imputation', imputation_transformer),\n",
    "    ('outlier_clipping', outlier_transformer),\n",
    "    ('normalization', normalize_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3502, 57)\n"
     ]
    }
   ],
   "source": [
    "data_preperation_pipelines.fit(X_train, y_train)\n",
    "label_transformer.fit(y_train)\n",
    "\n",
    "X_train_prepared, y_train_prepared = data_preperation_pipelines.transform(X_train), label_transformer.transform(y_train)\n",
    "X_validation_prepared, y_validation_prepared = data_preperation_pipelines.transform(X_val), label_transformer.transform(y_val)\n",
    "X_test_prepared, y_test_prepared = data_preperation_pipelines.transform(X_test), label_transformer.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  57 | elapsed:   16.3s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:   16.8s finished\n",
      "\n",
      "[2020-12-03 18:39:05] Features: 56/25 -- score: 0.6659051970302684[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  56 | elapsed:   15.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  56 | elapsed:   16.0s finished\n",
      "\n",
      "[2020-12-03 18:39:21] Features: 55/25 -- score: 0.6767561393489434[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  55 | elapsed:   15.1s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:   15.4s finished\n",
      "\n",
      "[2020-12-03 18:39:37] Features: 54/25 -- score: 0.6719017704169046[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  54 | elapsed:   14.9s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   15.5s finished\n",
      "\n",
      "[2020-12-03 18:39:53] Features: 53/25 -- score: 0.6784694460308395[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  53 | elapsed:   15.5s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  53 | elapsed:   16.0s finished\n",
      "\n",
      "[2020-12-03 18:40:09] Features: 52/25 -- score: 0.6836093660765277[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  52 | elapsed:   13.5s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:   14.2s finished\n",
      "\n",
      "[2020-12-03 18:40:23] Features: 51/25 -- score: 0.683894917190177[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  51 | elapsed:   13.2s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:   14.2s finished\n",
      "\n",
      "[2020-12-03 18:40:38] Features: 50/25 -- score: 0.6867504283266704[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n"
     ]
    }
   ],
   "source": [
    "select_features_wrapper(X_train_prepared, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features_filter(X_train_prepared, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_correlation_matrix(X_train_prepared, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X_trainn_prepared, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prepared.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ml_hw2)",
   "language": "python",
   "name": "pycharm-64409fe0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
